{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyN2q4GOlCnnqXTSghENGFBj"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a1b61424580544e098ca97e33e7208f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f23d3b6cca543839b018c50b7bbcdf7",
       "IPY_MODEL_f5637345d6ca4af38c9070411b0b984d",
       "IPY_MODEL_f7fd79cf8bcc48919cb2ad263ef10e98"
      ],
      "layout": "IPY_MODEL_d9bd11b83c7d4680883c48bc458e87cd"
     }
    },
    "3f23d3b6cca543839b018c50b7bbcdf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc9d3c03cd3540799675a0422e25aa99",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f9eda22393c45178fdcbda1091bc143",
      "value": "Map:â€‡100%"
     }
    },
    "f5637345d6ca4af38c9070411b0b984d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbdeffb8205f48ddaa04c27796053537",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fdade09828ad481a80c5e18881502bf2",
      "value": 1000
     }
    },
    "f7fd79cf8bcc48919cb2ad263ef10e98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8290af9aa7f43b0a99b90901c443cdd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bd91eb641a884da5832822b49d9cb5d2",
      "value": "â€‡1000/1000â€‡[00:00&lt;00:00,â€‡1202.27â€‡examples/s]"
     }
    },
    "d9bd11b83c7d4680883c48bc458e87cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9d3c03cd3540799675a0422e25aa99": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f9eda22393c45178fdcbda1091bc143": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbdeffb8205f48ddaa04c27796053537": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdade09828ad481a80c5e18881502bf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8290af9aa7f43b0a99b90901c443cdd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd91eb641a884da5832822b49d9cb5d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64ed49e765ea48999b92f170bd33924d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3006006d7a6b43eca024e53ae8dfe1ee",
       "IPY_MODEL_55cd48449d694592ac460a3e74d94983",
       "IPY_MODEL_4ab2cbb0503e4c83b5d48c257543ba32"
      ],
      "layout": "IPY_MODEL_aeef305d63704c6a9a9931e6da4330e4"
     }
    },
    "3006006d7a6b43eca024e53ae8dfe1ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7da6c8553cce45839b16c3c053a40128",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eb3576fd8fba4203a66cf9bb135bcf52",
      "value": "Downloadingâ€‡builderâ€‡script:â€‡100%"
     }
    },
    "55cd48449d694592ac460a3e74d94983": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79dd80e7992848eb8b6eb1feef289ee3",
      "max": 4203,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa8296a027a5476cb6b8ca1d423422ec",
      "value": 4203
     }
    },
    "4ab2cbb0503e4c83b5d48c257543ba32": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7f31a3428824de1926ef6a794b79d99",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_632c9f8d22544520b7214a05a64d3f82",
      "value": "â€‡4.20k/4.20kâ€‡[00:00&lt;00:00,â€‡295kB/s]"
     }
    },
    "aeef305d63704c6a9a9931e6da4330e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7da6c8553cce45839b16c3c053a40128": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb3576fd8fba4203a66cf9bb135bcf52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79dd80e7992848eb8b6eb1feef289ee3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa8296a027a5476cb6b8ca1d423422ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7f31a3428824de1926ef6a794b79d99": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632c9f8d22544520b7214a05a64d3f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Class 9: Transformers and LLMs\n",
    "\n",
    "In this class we will explore transformers and large language models. We will use Hugging Face to lead a few models to perform specific tasks. Then, we will fine-tune a pretrained model. Finally, we will look at LLM APIs (Google Gemini and OpenAI GPT-4)."
   ],
   "metadata": {
    "id": "eiP_ycbdHSuy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install datasets\n",
    "! pip install transformers[torch]\n",
    "! pip install accelerate -U\n",
    "! pip install -U transformers\n",
    "! pip install evaluate\n",
    "! pip install -q -U google-generativeai"
   ],
   "metadata": {
    "id": "3xGuQaUVU7WN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the first part, we will follow the Hugging Face (ðŸ¤—, for friends) introduction to transformers.\n",
    "\n",
    "For those interested, the ðŸ¤— NLP course also contains an introduction to transformers, which is a bit more detailed than what we discussed in class. You can find it here: [Hugging Face NLP Course - Introduction to Transformers](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt).\n",
    "\n",
    "Note that we are using Google Colab, as the necessary packages are pre-installed/realively easy to install. While it's possible to run this code locally, installing these packages on your system might not be straightforward and could require several attempts."
   ],
   "metadata": {
    "id": "MKR58rYxIMX-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first class we consider is Pipelines. In this first part of the notebook we will try a few of them, you can find the complete list of pipelines available from Hugging Face here: https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines"
   ],
   "metadata": {
    "id": "m0GcFd7dJw8F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmLKZ0gYFLa6"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentiment Analysis pipeline"
   ],
   "metadata": {
    "id": "u5985VmZUHKi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6b-N0oUFb86",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067755941,
     "user_tz": 240,
     "elapsed": 4303,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "f0bb34a9-e1b3-4aa8-9671-36933c81d1a2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The guide on pipelines also demonstrates how to adapt this code if you need to run the pipeline on a dataset.\n",
    "\n",
    "Additionally, take note of the warning that indicates we haven't specified which model to use for the task. The pipeline allows us to do this. For example, if we wanted to use a model trained on financial data, we could specify [https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis).\n",
    "\n",
    "How do you find new models? Search on ðŸ¤—!"
   ],
   "metadata": {
    "id": "0olU5cWVLJm7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier2 = pipeline(\"sentiment-analysis\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "classifier2(\"I've been waiting for a HuggingFace course my whole life.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AEbkvaNLwk3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067756964,
     "user_tz": 240,
     "elapsed": 1026,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "5bf48bb1-58f6-4822-e68b-bf5488763c0e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9998385906219482}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classifier(\"Operating profit from the oncology division totaled EUR 9.4 mn , up from EUR 8.7 mn in 2004 .\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pmfRBRaNiUm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067756964,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "66110458-b32e-4d65-9227-eb13bd6705e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9911375045776367}]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classifier2(\"Operating profit from the oncology division totaled EUR 9.4 mn , up from EUR 8.7 mn in 2004 .\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYRaIV--Nu2t",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067756964,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "3874fcbc-e18f-4a37-a8e4-bffae75f81e0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9997376799583435}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice how specific domains associate sentiment with specific terms through a different logic. The presence of \"oncology\" may cause a sentence to be interpreted as having a negative sentiment if we are using a general-purpose sentiment classifier.\n"
   ],
   "metadata": {
    "id": "Hl7HV7-5SGau"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A pipeline, like the ones we have used above, performs several operations under the hood. It downloads the model, preprocesses the text with tokenizers (ensuring the input matches the model's required format), passes the inputs through the model, and postprocesses the output into a format that's easily interpretable. For more details, see [this page](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt).\n"
   ],
   "metadata": {
    "id": "6rokJLzDRgWt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sequence Classification\n",
    "\n",
    "In the next example we will load another model ```bert-base-cased-finetuned-mrpc``` which has been finetuned on the Microsoft Research Paraphrase Corpus. This model allows us to understand whether a sentence paraphrases another one."
   ],
   "metadata": {
    "id": "mFGUJ8wtUMY4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_P-GPfORUHS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067759231,
     "user_tz": 240,
     "elapsed": 2269,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "c71d930f-53c5-44d9-b322-a50bf9046455"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zero-shot classification\n",
    "\n",
    "Zero-shot classification refers to the ability of a model to accurately classify data into categories it has never seen before during training. It leverages the model's understanding of language and context to make inferences about new or unseen categories based on its pre-existing knowledge. This approach is particularly useful in scenarios where labeled data is scarce or when it's impractical to retrain models for new categories. Essentially, zero-shot classification models use natural language understanding to generalize from seen to unseen categories without direct examples."
   ],
   "metadata": {
    "id": "Z3X7OKiAguHQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsT8IB6YatSN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067762131,
     "user_tz": 240,
     "elapsed": 2901,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "3995639a-295f-4f99-a2f9-d7326a1dd554"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938650727272034, 0.003273802110925317, 0.002861041808500886]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that since `facebook/bart-large-mnli` was trained on the MultiNLI dataset ([https://huggingface.co/datasets/nyu-mll/multi_nli](https://huggingface.co/datasets/nyu-mll/multi_nli)), it is potentially useful for performing Natural Language Inference (NLI). NLI involves, given a premise sentence and a hypothesis, determining whether they are entailed, in contradiction, or neither.\n"
   ],
   "metadata": {
    "id": "b7bPfBUYiakR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_to_classify = \"Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace is based in Ohio.\"\n",
    "candidate_labels = ['Contradicts', 'Entailment', 'Neutral']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJDpCI18f_OL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067763619,
     "user_tz": 240,
     "elapsed": 1490,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "36c4bf6e-03a4-402e-a4fc-5789384a3422"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace is based in Ohio.',\n",
       " 'labels': ['Contradicts', 'Entailment', 'Neutral'],\n",
       " 'scores': [0.8365421295166016, 0.1356203854084015, 0.027837563306093216]}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_to_classify = \"Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace's headquarters are situated in Manhattan.\"\n",
    "candidate_labels = ['Contradicts', 'Entailment', 'Neutral']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYuKtKipgVvs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067765636,
     "user_tz": 240,
     "elapsed": 2018,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "65f043f8-df60-4993-97e3-abff295d09a5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': \"Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace's headquarters are situated in Manhattan.\",\n",
       " 'labels': ['Entailment', 'Contradicts', 'Neutral'],\n",
       " 'scores': [0.5122519731521606, 0.37862467765808105, 0.10912329703569412]}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_to_classify = \"Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace's headquarters are not situated in Manhattan.\"\n",
    "candidate_labels = ['Contradicts', 'Entailment', 'Neutral']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj8zPx8ogeWJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067768160,
     "user_tz": 240,
     "elapsed": 2525,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "579af0e1-b2da-45de-955f-73f8979da065"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': \"Premise: The company HuggingFace is based in New York City. Hypothesis: HuggingFace's headquarters are not situated in Manhattan.\",\n",
       " 'labels': ['Contradicts', 'Entailment', 'Neutral'],\n",
       " 'scores': [0.8089037537574768, 0.1572469025850296, 0.033849332481622696]}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question Answering"
   ],
   "metadata": {
    "id": "_gb2tom5_F7-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bjg4uS51jNgz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067773681,
     "user_tz": 240,
     "elapsed": 5523,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "9d3a4646-04c5-4b74-bb3a-536d5087bce5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.6949766278266907, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"What are they discussing in this text?\",\n",
    "    context=\"In questioning the use of hydraulic fracturing in New York to help produce natural gas, you do not note that the technology has been employed and continuously improved for more than 50 years and that studies by the Environmental Protection Agency and the Ground Water Protection Council have not identified a single instance of groundwater contamination. Wells where fracturing is used are specially constructed to protect drinking water sources. Regulatory oversight is extensive. The fluids mostly water that are forced into a well to create pressure to fracture rock are pushed back out by the oil and gas flowing upward for safe processing. Protecting our water supplies is important, as are reductions in greenhouse gas emissions through use of clean-burning natural gas. Banning hydraulic fracturing would be unwarranted and shortsighted, preventing production of large amounts of natural gas that could directly benefit New York consumers for decades and create thousands of good jobs.\",\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjDNmfq5fO2_",
    "outputId": "d77691be-94e6-46d0-b1d3-ee7185d5d365",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067779314,
     "user_tz": 240,
     "elapsed": 5635,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.14289604127407074,\n",
       " 'start': 19,\n",
       " 'end': 86,\n",
       " 'answer': 'use of hydraulic fracturing in New York to help produce natural gas'}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning\n",
    "\n",
    "We have seen how to use the pretrained model directly for specific tasks. We can also use labels that we have available to fine-tune the model, that is, marginally adjust the parameters to improve performance in a specific task we are interested in.\n",
    "\n",
    "Here we will follow the [main tutorial](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer) and fine-tune a BERT model to classify reviews from Yelp."
   ],
   "metadata": {
    "id": "ge3i3PQgkA7g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "1LHwLrZOOg41"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ],
   "metadata": {
    "id": "JygIo04GNDMi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "id": "WiFwCu2fO8HT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067785068,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "9ffc6f0c-6f72-4dfc-f408-4b0ee06f3fb5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "# Now you can use DataFrame methods like .head()\n",
    "print(df.head())\n"
   ],
   "metadata": {
    "id": "H3Hd-LdnPgvN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067836720,
     "user_tz": 240,
     "elapsed": 51653,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "894f0d98-f1b5-4ee9-946c-e5f624edaadf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   label                                               text\n",
      "0      4  dr. goldberg offers everything i look for in a...\n",
      "1      1  Unfortunately, the frustration of being Dr. Go...\n",
      "2      3  Been going to Dr. Goldberg for over 10 years. ...\n",
      "3      3  Got a letter in the mail last week that said D...\n",
      "4      0  I don't know what Dr. Goldberg was like before...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[\"train\"][100]"
   ],
   "metadata": {
    "id": "ISSZ6uw-PxqT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067836721,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "4b60b082-9483-489c-dbb4-1daf113ed4bd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ],
   "metadata": {
    "id": "8Xu805OoTsId"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we will use a BERT model, we need to prepare the data accordingly: these models do not use as tokens full words, but they use subword tokenization. Here we will load the tokenizer from the ```transformers``` library."
   ],
   "metadata": {
    "id": "NI9cvBw4NXT-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n"
   ],
   "metadata": {
    "id": "3jyTX4f9NUQf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a1b61424580544e098ca97e33e7208f6",
      "3f23d3b6cca543839b018c50b7bbcdf7",
      "f5637345d6ca4af38c9070411b0b984d",
      "f7fd79cf8bcc48919cb2ad263ef10e98",
      "d9bd11b83c7d4680883c48bc458e87cd",
      "cc9d3c03cd3540799675a0422e25aa99",
      "9f9eda22393c45178fdcbda1091bc143",
      "bbdeffb8205f48ddaa04c27796053537",
      "fdade09828ad481a80c5e18881502bf2",
      "f8290af9aa7f43b0a99b90901c443cdd",
      "bd91eb641a884da5832822b49d9cb5d2"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067839014,
     "user_tz": 240,
     "elapsed": 2295,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "0adf93ec-d587-4206-e7f7-d862e2aaaa8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1b61424580544e098ca97e33e7208f6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ],
   "metadata": {
    "id": "2wJGa877OCI0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067840451,
     "user_tz": 240,
     "elapsed": 1439,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "33d33acb-cf87-44f5-e434-7400d45adf02"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ],
   "metadata": {
    "id": "OXAAgLp1QGI1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ],
   "metadata": {
    "id": "IfvHE8mFRAz9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "64ed49e765ea48999b92f170bd33924d",
      "3006006d7a6b43eca024e53ae8dfe1ee",
      "55cd48449d694592ac460a3e74d94983",
      "4ab2cbb0503e4c83b5d48c257543ba32",
      "aeef305d63704c6a9a9931e6da4330e4",
      "7da6c8553cce45839b16c3c053a40128",
      "eb3576fd8fba4203a66cf9bb135bcf52",
      "79dd80e7992848eb8b6eb1feef289ee3",
      "fa8296a027a5476cb6b8ca1d423422ec",
      "b7f31a3428824de1926ef6a794b79d99",
      "632c9f8d22544520b7214a05a64d3f82"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067848438,
     "user_tz": 240,
     "elapsed": 1965,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "9dcc4ac5-9c2f-47d4-d433-47091b9a9eb1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64ed49e765ea48999b92f170bd33924d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "id": "MxqFeGvxfT9I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ],
   "metadata": {
    "id": "aK_5T3erfpef"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "id": "OJdcy4T9frOa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711067849964,
     "user_tz": 240,
     "elapsed": 1530,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "fd3616a6-9812-4627-f3ff-e98d54632e47"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "id": "uImmcBwiftMY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068138784,
     "user_tz": 240,
     "elapsed": 288823,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "b904b162-a7ae-405e-bbb3-120a91b60c6f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 04:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.00583203125, metrics={'train_runtime': 288.5708, 'train_samples_per_second': 10.396, 'train_steps_per_second': 1.3, 'total_flos': 789354427392000.0, 'train_loss': 1.00583203125, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ],
   "metadata": {
    "id": "Xr4U28K9hhSX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068278938,
     "user_tz": 240,
     "elapsed": 30889,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "adfa7213-900f-4f93-efb0-f7af5eab81ac"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls \"/content/drive/MyDrive/Data_course\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08167qVmoTsw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068497780,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "bd2c37d9-d2e2-419e-8301-37d13182557e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "book_reviews.csv  Songs.pkl\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained('/content/drive/MyDrive/Data_course/Fine-tuned_model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/Data_course/Fine-tuned_model')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcLN93XCntjB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068795274,
     "user_tz": 240,
     "elapsed": 7667,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "1c36001f-2376-4f73-be6a-b4999de2008b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Data_course/Fine-tuned_model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Data_course/Fine-tuned_model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Data_course/Fine-tuned_model/vocab.txt',\n",
       " '/content/drive/MyDrive/Data_course/Fine-tuned_model/added_tokens.json',\n",
       " '/content/drive/MyDrive/Data_course/Fine-tuned_model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the path to your saved model and tokenizer\n",
    "model_directory = \"/content/drive/MyDrive/Data_course/Fine-tuned_model\"  # Adjust this path\n",
    "\n",
    "# Create a pipeline\n",
    "# The model and tokenizer will be automatically loaded from the specified directory\n",
    "classifier = pipeline(\"text-classification\", model=model_directory, tokenizer=model_directory)\n",
    "\n",
    "# Example usage\n",
    "text = \"The restaurant was terrible!!\"\n",
    "predictions = classifier(text)\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiAn1P1Oovh8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068634138,
     "user_tz": 240,
     "elapsed": 1919,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "bb729976-2566-46e6-e4f8-f357a18c9b7d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.7803637981414795}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"I will come back to try the pizza.\"\n",
    "predictions = classifier(text)\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRwQwZpAo4Gn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068666017,
     "user_tz": 240,
     "elapsed": 321,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "629411d7-98f3-4d5c-9b8b-996101af0b42"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': 'LABEL_2', 'score': 0.35370907187461853}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Google GEMINI"
   ],
   "metadata": {
    "id": "l4OsLkqelOFo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll explore the use of Large Language Model (LLM) APIs, specifically focusing on Google Gemini, which offers a free tier with a limited number of requests.\n",
    "\n",
    "- For a quick start, check out this [Tutorial](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb#scrollTo=ab9ASynfcIZn).\n",
    "\n",
    "- To access the API, visit [API](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "- For information on pricing, see [Pricing](https://ai.google.dev/pricing).\n"
   ],
   "metadata": {
    "id": "5dX-gpK3mr9Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q -U google-generativeai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e97ZUjcElREM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711068946075,
     "user_tz": 240,
     "elapsed": 39512,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "7820cb7c-177c-4321-c47b-5f973a98f3f4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/137.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91mâ”â”â”â”â”â”â”â”\u001B[0m\u001B[91mâ•¸\u001B[0m\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m30.7/137.4 kB\u001B[0m \u001B[31m703.5 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[91mâ•¸\u001B[0m\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m61.4/137.4 kB\u001B[0m \u001B[31m720.0 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[91mâ•¸\u001B[0m\u001B[90mâ”â”â”â”\u001B[0m \u001B[32m122.9/137.4 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m137.4/137.4 kB\u001B[0m \u001B[31m987.4 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ],
   "metadata": {
    "id": "1dz9F9FIlUxY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import userdata"
   ],
   "metadata": {
    "id": "KDudWW5MlWX9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ],
   "metadata": {
    "id": "k-DzU8UTlaHI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ],
   "metadata": {
    "id": "k3_CaZMhreWQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = model.generate_content([\"Who is the villain in the following text? 1:Answer identifying the villain, if there is one clearly mentioned. 2:If the villain is only implicitly suggested, mention them. 3: If no villain is mentioned (not directly nor implicitly) say that there is no villain. TEXT: In questioning the use of hydraulic fracturing in New York to help produce natural gas, you do not note that the technology has been employed and continuously improved for more than 50 years and that studies by the Environmental Protection Agency and the Ground Water Protection Council have not identified a single instance of groundwater contamination. Wells where fracturing is used are specially constructed to protect drinking water sources. Regulatory oversight is extensive. The fluids mostly water that are forced into a well to create pressure to fracture rock are pushed back out by the oil and gas flowing upward for safe processing. Protecting our water supplies is important, as are reductions in greenhouse gas emissions through use of clean-burning natural gas. Banning hydraulic fracturing would be unwarranted and shortsighted, preventing production of large amounts of natural gas that could directly benefit New York consumers for decades and create thousands of good jobs.\"], stream=False)\n",
    "response.resolve()"
   ],
   "metadata": {
    "id": "IABUo_HysZ2E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "to_markdown(response.text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "id": "9biJ5Rb3sq0D",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711069970849,
     "user_tz": 240,
     "elapsed": 135,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "2cfd5cf3-948e-495e-e7aa-2c07835e7980"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "> No villain is mentioned in the text."
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenAI\n",
    "\n",
    "We can also look at OpenAI, which also offers an API to interact with their models.\n",
    "\n",
    "[API OpenAI](https://platform.openai.com/api-keys)\n",
    "\n",
    "[Pricing](https://openai.com/pricing)\n",
    "\n",
    "[Embeddings](https://platform.openai.com/docs/guides/embeddings/use-cases)\n"
   ],
   "metadata": {
    "id": "zovAn-farRtL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install OpenAI"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeaTe49vv_RZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711070533586,
     "user_tz": 240,
     "elapsed": 15032,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "178edd35-b2d6-4d93-de4d-c29e898b2eb5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting OpenAI\n",
      "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m262.4/262.4 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from OpenAI) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from OpenAI)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m75.6/75.6 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from OpenAI) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->OpenAI) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->OpenAI)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m77.8/77.8 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (2.16.3)\n",
      "Installing collected packages: h11, httpcore, httpx, OpenAI\n",
      "Successfully installed OpenAI-1.14.2 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "api_key = userdata.get('OpenAI')\n"
   ],
   "metadata": {
    "id": "C9bul2Wyxa7z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Check if the API key was retrieved successfully\n",
    "if api_key is None:\n",
    "    print(\"API key not found.\")\n",
    "else:\n",
    "    # Define the API endpoint for chat completions\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    # Headers for the API request\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Data payload for the API request\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Print the response content\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\", response.text)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AoSNkvDxJ5a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711070889143,
     "user_tz": 240,
     "elapsed": 421,
     "user": {
      "displayName": "Tiziano Rotesi",
      "userId": "10236335377536012302"
     }
    },
    "outputId": "6c477413-8c86-4134-da4e-8916e497409a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: 429 {\n",
      "    \"error\": {\n",
      "        \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\",\n",
      "        \"type\": \"insufficient_quota\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"insufficient_quota\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ]
  }
 ]
}
